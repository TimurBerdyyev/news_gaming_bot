import feedparser
import telegram
import schedule
import time
from datetime import datetime, timedelta
from huggingface_hub import InferenceClient
from langdetect import detect
import os
import json
import logging
import signal
import threading
import re
from urllib.parse import urlparse, urlunparse, parse_qsl, urlencode
import requests
from telegram.error import RetryAfter, TimedOut, NetworkError
from dotenv import load_dotenv
import asyncio
import html as html_lib
from operator import itemgetter  # –î–ª—è —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∏ —Å–∫–∏–¥–æ–∫

# –ó–∞–≥—Ä—É–∑–∫–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è –∏–∑ .env
load_dotenv()

# –ö–æ–Ω—Å—Ç–∞–Ω—Ç—ã –∏–∑ –æ–∫—Ä—É–∂–µ–Ω–∏—è
BOT_TOKEN = os.getenv('BOT_TOKEN', '')
CHANNEL_ID = os.getenv('CHANNEL_ID', '')
HF_TOKEN = os.getenv('HF_TOKEN', '')
GENERATE_VIA_HF = os.getenv('GENERATE_VIA_HF', '0')
USE_GROQ = os.getenv('USE_GROQ', '0')
GROQ_API_KEY = os.getenv('GROQ_API_KEY', '')
GROQ_MODEL = os.getenv('GROQ_MODEL', 'llama-3.1-8b-instant')

# –í–∞–ª–∏–¥–∞—Ü–∏—è –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö
if not BOT_TOKEN:
    raise RuntimeError('–ù–µ —É–∫–∞–∑–∞–Ω BOT_TOKEN –≤ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è')
if not CHANNEL_ID:
    raise RuntimeError('–ù–µ —É–∫–∞–∑–∞–Ω CHANNEL_ID –≤ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è')

# –ù–∞—Å—Ç—Ä–æ–π–∫–∏
MAX_TELEGRAM_MESSAGE = 4096
MAX_CAPTION = 1024
SUMMARY_MAX_LEN = 2000
REQUEST_TIMEOUT_SEC = 20
MAX_RETRIES = 3
BACKOFF_BASE_SEC = 2
LAST_POSTS_FILE = 'last_posts.json'
USER_AGENT = 'NewsGameBot/1.0 (+https://t.me/your_bot)'
MIN_DISCOUNT_PERCENT = 10  # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è —Å–∫–∏–¥–∫–∞ –¥–ª—è –ø–æ–∫–∞–∑–∞

# –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
LOG_LEVEL = os.getenv('LOG_LEVEL', 'INFO')
POST_INTERVAL_HOURS = int(os.getenv('POST_INTERVAL_HOURS', '6'))
MIN_DISCOUNT_PERCENT = int(os.getenv('MIN_DISCOUNT_PERCENT', str(MIN_DISCOUNT_PERCENT)))
MAX_DEALS = int(os.getenv('MAX_DEALS', '3'))
DEAL_TTL_HOURS = int(os.getenv('DEAL_TTL_HOURS', '12'))

# –•—Ä–∞–Ω–∏–ª–∏—â–∞ ETag/Last-Modified –¥–ª—è RSS
rss_etags: dict[str, str] = {}
rss_last_modified: dict[str, str] = {}

# –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
logging.basicConfig(
    level=getattr(logging, LOG_LEVEL.upper(), logging.INFO),
    format='%(asctime)s [%(levelname)s] %(message)s'
)

# TTL-–∫—ç—à —Å–∫–∏–¥–æ–∫
from time import monotonic
_deal_timestamps: dict[str, float] = {}

def _deal_cache_get(key: str):
    ts = _deal_timestamps.get(key)
    if ts is None:
        return None
    if (monotonic() - ts) > DEAL_TTL_HOURS * 3600:
        deal_cache.pop(key, None)
        _deal_timestamps.pop(key, None)
        return None
    return deal_cache.get(key)

def _deal_cache_set(key: str, value):
    deal_cache[key] = value
    _deal_timestamps[key] = monotonic()

# –£–ª—É—á—à–µ–Ω–Ω—ã–π –≤—ã–±–æ—Ä –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
import math

def _score_image_url(url: str) -> int:
    url_l = url.lower()
    score = 0
    # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç —Ñ–æ—Ä–º–∞—Ç–æ–≤
    if url_l.endswith('.webp'):
        score += 3
    elif url_l.endswith('.jpg') or url_l.endswith('.jpeg'):
        score += 2
    elif url_l.endswith('.png'):
        score += 1
    # –≠–≤—Ä–∏—Å—Ç–∏–∫–∞: –±–æ–ª—å—à–∏–µ —á–∏—Å–ª–∞ –≤ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞ ‚Äî –≤–æ–∑–º–æ–∂–Ω–æ, –±–æ–ª—å—à–æ–π —Ä–∞–∑–º–µ—Ä
    for hint in ['1080', '1200', '1440', '1600', '1920', '2048']:
        if hint in url_l:
            score += 2
    return score

def _head_ok(url: str) -> bool:
    try:
        r = requests.head(url, timeout=5, allow_redirects=True)
        ct = r.headers.get('content-type', '')
        return r.status_code == 200 and ('image/' in ct)
    except Exception:
        return False

def get_image_url(entry) -> str | None:
    candidates: list[str] = []
    try:
        media = getattr(entry, 'media_content', None)
        if media and isinstance(media, list):
            for m in media:
                u = m.get('url') if isinstance(m, dict) else None
                if u:
                    candidates.append(u)
        thumbs = getattr(entry, 'media_thumbnail', None)
        if thumbs and isinstance(thumbs, list):
            for m in thumbs:
                u = m.get('url') if isinstance(m, dict) else None
                if u:
                    candidates.append(u)
        enclosures = getattr(entry, 'enclosures', None)
        if enclosures and isinstance(enclosures, list):
            for enc in enclosures:
                u = enc.get('href') if isinstance(enc, dict) else None
                if u:
                    candidates.append(u)
        content = entry.get('content', []) if isinstance(entry, dict) else []
        for c in content:
            val = c.get('value', '')
            if 'img ' in val:
                import re as _re
                m = _re.search(r'src=["\']([^"\']+)', val)
                if m:
                    candidates.append(m.group(1))
    except Exception:
        pass

    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –ø–æ —ç–≤—Ä–∏—Å—Ç–∏—á–µ—Å–∫–æ–º—É —Å–∫–æ—Ä—É
    candidates = list(dict.fromkeys(candidates))  # —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ
    candidates.sort(key=_score_image_url, reverse=True)

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º HEAD —É —Ç–æ–ø-3
    for u in candidates[:3]:
        if _head_ok(u):
            return u
    # –ï—Å–ª–∏ HEAD –Ω–µ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–∞–µ—Ç, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –ª—É—á—à–∏–π –Ω–∞–π–¥–µ–Ω–Ω—ã–π
    return candidates[0] if candidates else None

# –°–ø–∏—Å–æ–∫ RSS-—Ñ–∏–¥–æ–≤
rss_feeds = [
    'https://www.gameinformer.com/rss',
    'https://www.playground.ru/rss/news.xml',
]

# –•—Ä–∞–Ω–∏–ª–∏—â–µ –¥–ª—è –ø–æ—Å–ª–µ–¥–Ω–∏—Ö –Ω–æ–≤–æ—Å—Ç–µ–π –∏ –∫—ç—à —Å–∫–∏–¥–æ–∫
last_posts = {}
deal_cache = {}  # –ö—ç—à –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ API —Å–∫–∏–¥–æ–∫

stop_event = threading.Event()

def load_last_posts():
    try:
        if os.path.exists(LAST_POSTS_FILE):
            with open(LAST_POSTS_FILE, 'r', encoding='utf-8') as f:
                data = json.load(f)
                if isinstance(data, dict):
                    return data
    except Exception as e:
        logging.warning(f'–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å {LAST_POSTS_FILE}: {e}')
    return {}

def save_last_posts():
    try:
        with open(LAST_POSTS_FILE, 'w', encoding='utf-8') as f:
            json.dump(last_posts, f, ensure_ascii=False, indent=2)
    except Exception as e:
        logging.error(f'–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å {LAST_POSTS_FILE}: {e}')

def normalize_url(url: str) -> str:
    try:
        parsed = urlparse(url)
        query_params = [(k, v) for k, v in parse_qsl(parsed.query) if not k.lower().startswith('utm_')]
        clean_query = urlencode(query_params)
        normalized = parsed._replace(query=clean_query, fragment='')
        return urlunparse(normalized)
    except Exception:
        return url

def short_source_anchor(url: str) -> str:
    try:
        netloc = urlparse(url).netloc
        if netloc.startswith('www.'):
            netloc = netloc[4:]
        return f'<a href="{html_lib.escape(url)}">{html_lib.escape(netloc)}</a>'
    except Exception:
        return html_lib.escape(url)

def split_message(text: str, limit: int = MAX_TELEGRAM_MESSAGE) -> list:
    if len(text) <= limit:
        return [text[:limit]]
    parts = []
    start = 0
    while start < len(text):
        end = min(start + limit, len(text))
        chunk = text[start:end]
        last_nl = chunk.rfind('\n')
        last_sp = chunk.rfind(' ')
        cut = max(last_nl, last_sp)
        if cut > 0:
            parts.append(chunk[:cut].rstrip())
            start += cut
        else:
            parts.append(chunk)
            start = end
    return parts

def request_with_retries(url: str) -> requests.Response | None:
    headers = {'User-Agent': USER_AGENT}
    # –£—Å–ª–æ–≤–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã –ø–æ ETag/Last-Modified
    etag = rss_etags.get(url)
    last_mod = rss_last_modified.get(url)
    if etag:
        headers['If-None-Match'] = etag
    if last_mod:
        headers['If-Modified-Since'] = last_mod
    for attempt in range(1, MAX_RETRIES + 1):
        try:
            resp = requests.get(url, headers=headers, timeout=REQUEST_TIMEOUT_SEC)
            if resp.status_code == 200:
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º ETag/Last-Modified –¥–ª—è –±—É–¥—É—â–∏—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
                et = resp.headers.get('ETag')
                lm = resp.headers.get('Last-Modified')
                if et:
                    rss_etags[url] = et
                if lm:
                    rss_last_modified[url] = lm
                return resp
            if resp.status_code == 304:
                logging.info(f'RSS –Ω–µ –∏–∑–º–µ–Ω–∏–ª—Å—è (304): {url}')
                return None
            logging.warning(f'HTTP {resp.status_code} –¥–ª—è {url} (–ø–æ–ø—ã—Ç–∫–∞ {attempt}/{MAX_RETRIES})')
        except requests.RequestException as e:
            logging.warning(f'–û—à–∏–±–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ {url}: {e} (–ø–æ–ø—ã—Ç–∫–∞ {attempt}/{MAX_RETRIES})')
        time.sleep(BACKOFF_BASE_SEC ** attempt)
    return None

def is_game_related(title: str, summary: str) -> bool:
    game_keywords = [
        'game', 'gaming', 'video game', 'nintendo', 'playstation', 'xbox', 
        'pc game', 'steam', 'epic games', 'assassin', 'battlefield', 
        'final fantasy', 'hollow knight', 'dying light'
    ]
    non_game_keywords = [
        'blu-ray', 'movie', 'film', 'collection', 'lego', 'soundcore', 
        'anker', 'casino', 'scorsese', 'batman anniversary', 'zombie movie'
    ]
    title_lower = title.lower()
    summary_lower = summary.lower()
    has_game_keyword = any(keyword in title_lower or keyword in summary_lower for keyword in game_keywords)
    has_non_game_keyword = any(keyword in title_lower or keyword in summary_lower for keyword in non_game_keywords)
    return has_game_keyword and not has_non_game_keyword

def extract_game_title(title: str) -> str:
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –Ω–∞–∑–≤–∞–Ω–∏–µ –∏–≥—Ä—ã –∏–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞ –Ω–æ–≤–æ—Å—Ç–∏."""
    title = title.strip()
    # –£–¥–∞–ª—è–µ–º –∫–∞–≤—ã—á–∫–∏, –µ—Å–ª–∏ –Ω–∞–∑–≤–∞–Ω–∏–µ –≤ –Ω–∏—Ö
    if title.startswith('"') and title.endswith('"'):
        title = title[1:-1]
    # –ë–µ—Ä–µ–º –¥–æ –¥–≤–æ–µ—Ç–æ—á–∏—è –∏–ª–∏ –ø–µ—Ä–≤—ã–µ 4 —Å–ª–æ–≤–∞
    if ':' in title:
        return title.split(':', 1)[0].strip()
    words = title.split()
    return ' '.join(words[:4]).strip()

def check_best_deal(game_title: str) -> str | None:
    """
    –ü—Ä–æ–≤–µ—Ä—è–µ—Ç —Å–∫–∏–¥–∫–∏ –Ω–∞ –∏–≥—Ä—É —á–µ—Ä–µ–∑ CheapShark API, –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ª—É—á—à—É—é.
    –ö—ç—à–∏—Ä—É–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç, —á—Ç–æ–±—ã –Ω–µ –∑–∞–ø—Ä–∞—à–∏–≤–∞—Ç—å –ø–æ–≤—Ç–æ—Ä–Ω–æ.
    """
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à
    if game_title in deal_cache:
        return deal_cache[game_title]

    search_url = f"https://www.cheapshark.com/api/1.0/deals?storeID=1,2,3,7,11,15,23,25,29,30,31&title={game_title.replace(' ', '+')}&limit=10"
    try:
        response = requests.get(search_url, timeout=10)
        if response.status_code == 200:
            data = response.json()
            if data:
                deals = []
                for deal in data:
                    sale_price = float(deal['salePrice'])
                    normal_price = float(deal['normalPrice'])
                    if sale_price < normal_price:
                        discount = int((1 - sale_price / normal_price) * 100)
                        if discount >= MIN_DISCOUNT_PERCENT:
                            store_id = deal['storeID']
                            # –ú–∞–ø–ø–∏–Ω–≥ ID –º–∞–≥–∞–∑–∏–Ω–æ–≤ –Ω–∞ –Ω–∞–∑–≤–∞–Ω–∏—è
                            store_names = {
                                '1': 'Steam',
                                '2': 'GamersGate',
                                '3': 'GreenManGaming',
                                '7': 'GOG',
                                '11': 'Humble Bundle',
                                '15': 'Fanatical',
                                '23': 'GameBillet',
                                '25': 'Epic Games',
                                '29': '2game',
                                '30': 'IndieGala',
                                '31': 'Blizzard'
                            }
                            store_name = store_names.get(store_id, f'Store {store_id}')
                            deal_link = f"https://www.cheapshark.com/redirect?dealID={deal['dealID']}"
                            deals.append({
                                'discount': discount,
                                'store': store_name,
                                'price': sale_price,
                                'link': deal_link
                            })
                
                if deals:
                    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —Å–∫–∏–¥–∫–µ (—É–±—ã–≤–∞–Ω–∏–µ)
                    best_deal = max(deals, key=itemgetter('discount'))
                    deal_text = f"üî• –õ—É—á—à–∞—è —Å–∫–∏–¥–∫–∞ {best_deal['discount']}% –≤ {best_deal['store']} –∑–∞ ${best_deal['price']}! –ö—É–ø–∏—Ç—å: {best_deal['link']}"
                    deal_cache[game_title] = deal_text  # –ö—ç—à–∏—Ä—É–µ–º
                    return deal_text
    except Exception as e:
        logging.warning(f"–û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ —Å–∫–∏–¥–æ–∫ –¥–ª—è '{game_title}': {e}")
    deal_cache[game_title] = None  # –ö—ç—à–∏—Ä—É–µ–º –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–µ —Å–∫–∏–¥–æ–∫
    return None

def check_top_deals(game_title: str, limit: int | None = None) -> list[str]:
    """
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –¥–æ limit –ª—É—á—à–∏—Ö —Å–∫–∏–¥–æ–∫ —Å –Ω–∞–∑–≤–∞–Ω–∏—è–º–∏ –º–∞–≥–∞–∑–∏–Ω–æ–≤ –∏ —Å—Å—ã–ª–∫–∞–º–∏.
    –ö—ç—à–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –ø–æ –Ω–∞–∑–≤–∞–Ω–∏—é –∏–≥—Ä—ã.
    """
    if limit is None:
        limit = MAX_DEALS
    cache_key = f"top:{game_title}:{limit}"
    cached = _deal_cache_get(cache_key)
    if cached is not None:
        return cached

    search_url = (
        "https://www.cheapshark.com/api/1.0/deals?"
        "storeID=1,2,3,7,11,15,23,25,29,30,31&"
        f"title={game_title.replace(' ', '+')}&limit=20"
    )
    deals_found: list[dict] = []
    try:
        response = requests.get(search_url, timeout=10)
        if response.status_code == 200:
            data = response.json() or []
            store_names = {
                '1': 'Steam', '2': 'GamersGate', '3': 'GreenManGaming', '7': 'GOG',
                '11': 'Humble Bundle', '15': 'Fanatical', '23': 'GameBillet', '25': 'Epic Games',
                '29': '2game', '30': 'IndieGala', '31': 'Blizzard'
            }
            for deal in data:
                try:
                    sale_price = float(deal.get('salePrice', '0') or 0)
                    normal_price = float(deal.get('normalPrice', '0') or 0)
                    if sale_price <= 0 or normal_price <= 0 or sale_price >= normal_price:
                        continue
                    discount = int((1 - sale_price / normal_price) * 100)
                    if discount < MIN_DISCOUNT_PERCENT:
                        continue
                    store_id = str(deal.get('storeID', ''))
                    store_name = store_names.get(store_id, f'Store {store_id}')
                    deal_link = f"https://www.cheapshark.com/redirect?dealID={deal.get('dealID')}"
                    deals_found.append({
                        'discount': discount,
                        'store': store_name,
                        'price': sale_price,
                        'link': deal_link,
                    })
                except Exception:
                    continue
    except Exception as e:
        logging.warning(f"–û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ —Å–∫–∏–¥–æ–∫ –¥–ª—è '{game_title}': {e}")

    deal_lines: list[str] = []
    if deals_found:
        deals_found.sort(key=lambda d: d['discount'], reverse=True)
        for d in deals_found[:limit]:
            deal_lines.append(
                f"üí∏ {d['store']}: ‚àí{d['discount']}% –∑–∞ ${d['price']} ‚Äî <a href=\"{html_lib.escape(d['link'])}\">–∫—É–ø–∏—Ç—å</a>"
            )

    _deal_cache_set(cache_key, deal_lines)
    return deal_lines

async def send_telegram_message(bot: telegram.Bot, chat_id: str, text: str):
    logging.info(f"–û—Ç–ø—Ä–∞–≤–∫–∞ —Å–æ–æ–±—â–µ–Ω–∏—è –≤ {chat_id}: {text[:50]}...")
    try:
        await bot.send_message(
            chat_id=chat_id,
            text=text,
            disable_web_page_preview=False
        )
        logging.info("–°–æ–æ–±—â–µ–Ω–∏–µ —É—Å–ø–µ—à–Ω–æ –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–æ")
        await asyncio.sleep(2)  # –ó–∞–¥–µ—Ä–∂–∫–∞ 2 —Å–µ–∫—É–Ω–¥—ã
    except Exception as e:
        logging.error(f"–û—à–∏–±–∫–∞ –æ—Ç–ø—Ä–∞–≤–∫–∏: {e}")
        raise e

def process_news(title, summary, link):
    try:
        safe_link = normalize_url(link or '')
        message_html = f"<b>üì∞ {html_lib.escape(title)}</b>\n\n–ò—Å—Ç–æ—á–Ω–∏–∫: {short_source_anchor(safe_link)}"
        
        # –ö–ª–∏–µ–Ω—Ç Hugging Face (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –æ—Ç–∫–ª—é—á—ë–Ω, –µ—Å–ª–∏ —Ñ–ª–∞–≥ –Ω–µ –≤—ã—Å—Ç–∞–≤–ª–µ–Ω)
        client = None
        if HF_TOKEN and GENERATE_VIA_HF == '1':
            client = InferenceClient(token=HF_TOKEN)

        if client:
            summary = (summary or '')[:SUMMARY_MAX_LEN]
            text_for_lang = f"{title}\n{summary}".strip()
            need_translate = False
            if text_for_lang and len(text_for_lang) >= 20:
                try:
                    lang = detect(text_for_lang)
                    need_translate = (lang != 'ru')
                except Exception:
                    need_translate = False

            if need_translate:
                try:
                    translation = client.translation(
                        text=summary or title,
                        model="Helsinki-NLP/opus-mt-en-ru",
                    )
                    summary = translation['translation_text'] if isinstance(translation, dict) else translation
                except Exception as e:
                    logging.warning(f'–û—à–∏–±–∫–∞ –ø–µ—Ä–µ–≤–æ–¥–∞ HF: {e}')
                    summary = summary or title

            summary_text = summary
            try:
                summarization = client.summarization(
                    text=summary,
                    model="facebook/bart-large-cnn",
                )
                summary_text = summarization['summary_text'] if isinstance(summarization, dict) else summarization
            except Exception as e:
                logging.warning(f'–û—à–∏–±–∫–∞ —Å–∞–º–º–∞—Ä–∏ HF: {e}')
                summary_text = summary
            
            message_html = f"<b>üì∞ {html_lib.escape(title)}</b>\n\n{html_lib.escape(summary_text)}\n\n–ò—Å—Ç–æ—á–Ω–∏–∫: {short_source_anchor(safe_link)}"
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–∫–∏–¥–æ–∫ (—Ç–µ–ø–µ—Ä—å –≤—Å–µ–≥–¥–∞ –ø—Ä–æ–±—É–µ–º –¥–æ–±–∞–≤–∏—Ç—å, –µ—Å–ª–∏ –Ω–∞–π–¥–µ–Ω—ã)
        game_title = extract_game_title(title)
        deal_lines = check_top_deals(game_title, MAX_DEALS)
        if deal_lines:
            message_html += "\n\n" + "\n".join(deal_lines)

        return message_html
    except Exception as e:
        logging.error(f"–û—à–∏–±–∫–∞ HF API: {e}")
        return f"<b>üì∞ {html_lib.escape(title)}</b>\n\n–ò—Å—Ç–æ—á–Ω–∏–∫: {short_source_anchor(safe_link)}"

def generate_post_via_hf(title: str, summary: str) -> str | None:
    if not client:
        return None
    try:
        prompt = (
            "–°—Ñ–æ—Ä–º—É–ª–∏—Ä—É–π –∫–æ—Ä–æ—Ç–∫–∏–π –Ω–æ–≤–æ—Å—Ç–Ω–æ–π –ø–æ—Å—Ç (2-4 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è) –ø–æ —Ç–µ–º–µ –≤–∏–¥–µ–æ–∏–≥—Ä –Ω–∞ —Ä—É—Å—Å–∫–æ–º. "
            "–ò–∑–±–µ–≥–∞–π –∫–ª–∏—à–µ –∏ –≤–æ–¥—ã, –±—É–¥—å –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–º –∏ –Ω–µ–π—Ç—Ä–∞–ª—å–Ω—ã–º. "
            f"–ó–∞–≥–æ–ª–æ–≤–æ–∫: {title}\n–¢–µ–∫—Å—Ç: {summary[:800]}"
        )
        out = client.text_generation(
            prompt=prompt,
            model="tiiuae/falcon-7b-instruct",
            max_new_tokens=120,
            temperature=0.6,
            top_p=0.9,
        )
        if isinstance(out, dict):
            return out.get('generated_text')
        return str(out)
    except Exception as e:
        logging.warning(f"HF text_generation –æ—à–∏–±–∫–∞: {e}")
        return None

def generate_post_via_groq(title: str, summary: str) -> str | None:
    if USE_GROQ != '1' or not GROQ_API_KEY:
        return None
    try:
        url = 'https://api.groq.com/openai/v1/chat/completions'
        headers = {
            'Authorization': f'Bearer {GROQ_API_KEY}',
            'Content-Type': 'application/json',
        }
        prompt = (
            "–°—Ñ–æ—Ä–º—É–ª–∏—Ä—É–π –∫–æ—Ä–æ—Ç–∫–∏–π –Ω–æ–≤–æ—Å—Ç–Ω–æ–π –ø–æ—Å—Ç (2-4 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è) –ø–æ —Ç–µ–º–µ –≤–∏–¥–µ–æ–∏–≥—Ä –Ω–∞ —Ä—É—Å—Å–∫–æ–º. "
            "–ò–∑–±–µ–≥–∞–π –∫–ª–∏—à–µ –∏ –≤–æ–¥—ã, –±—É–¥—å –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–º –∏ –Ω–µ–π—Ç—Ä–∞–ª—å–Ω—ã–º. "
            f"–ó–∞–≥–æ–ª–æ–≤–æ–∫: {title}\n–¢–µ–∫—Å—Ç: {summary[:800]}"
        )
        payload = {
            'model': GROQ_MODEL,
            'messages': [
                {'role': 'system', 'content': '–¢—ã –ø–æ–º–æ—â–Ω–∏–∫-—Ä–µ–¥–∞–∫—Ç–æ—Ä –Ω–æ–≤–æ—Å—Ç–µ–π –æ–± –∏–≥—Ä–∞—Ö. –ü–∏—à–∏ –∫—Ä–∞—Ç–∫–æ –∏ –ø–æ –¥–µ–ª—É.'},
                {'role': 'user', 'content': prompt},
            ],
            'temperature': 0.6,
            'top_p': 0.9,
            'max_tokens': 200,
        }
        resp = requests.post(url, headers=headers, json=payload, timeout=REQUEST_TIMEOUT_SEC)
        if resp.status_code != 200:
            logging.warning(f'Groq API {resp.status_code}: {resp.text[:200]}')
            return None
        data = resp.json()
        content = data.get('choices', [{}])[0].get('message', {}).get('content')
        return content
    except Exception as e:
        logging.warning(f'Groq API –æ—à–∏–±–∫–∞: {e}')
        return None

async def fetch_and_post_news():
    bot = telegram.Bot(token=BOT_TOKEN)
    async with bot:
        news_sent = 0
        for feed_url in rss_feeds:
            if news_sent >= 1:
                break
            for attempt in range(1, MAX_RETRIES + 1):
                try:
                    resp = request_with_retries(feed_url)
                    if not resp:
                        logging.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å RSS-–ª–µ–Ω—Ç—É {feed_url} –ø–æ—Å–ª–µ {MAX_RETRIES} –ø–æ–ø—ã—Ç–æ–∫")
                        continue
                    feed = feedparser.parse(resp.content)
                    for entry in feed.entries:
                        if news_sent >= 1:
                            break
                        try:
                            if 'published_parsed' in entry and entry.published_parsed:
                                pub_date = datetime(*entry.published_parsed[:6])
                                if pub_date < datetime.now() - timedelta(hours=24):
                                    continue

                            title = entry.get('title', '–ë–µ–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞')
                            summary = entry.get('summary', entry.get('description', ''))
                            link = entry.get('link', '')

                            post_key = f"{title}_{normalize_url(link)}"
                            if post_key in last_posts:
                                continue

                            message_html = process_news(title, summary, link)
                            image_url = get_image_url(entry)

                            # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–±—É–µ–º Groq (–µ—Å–ª–∏ –≤–∫–ª—é—á—ë–Ω), –∑–∞—Ç–µ–º HF (–µ—Å–ª–∏ –≤–∫–ª—é—á—ë–Ω)
                            generated = generate_post_via_groq(title, summary)
                            if not generated and GENERATE_VIA_HF == '1':
                                generated = generate_post_via_hf(title, summary) if 'generate_post_via_hf' in globals() else None

                            if generated:
                                safe_link = normalize_url(link or '')
                                # –ù–µ –¥—É–±–ª–∏—Ä—É–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫: –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç + –∏—Å—Ç–æ—á–Ω–∏–∫
                                message_html = (
                                    f"{html_lib.escape(generated)}\n\n"
                                    f"–ò—Å—Ç–æ—á–Ω–∏–∫: {short_source_anchor(safe_link)}"
                                )
                                # –î–æ–±–∞–≤–ª—è–µ–º —Å–∫–∏–¥–∫–∏, –µ—Å–ª–∏ –Ω–∞–π–¥–µ–Ω—ã
                                game_title = extract_game_title(title)
                                deal_lines = check_top_deals(game_title, MAX_DEALS)
                                if deal_lines:
                                    message_html += "\n\n" + "\n".join(deal_lines)
                            else:
                                # –ë–∞–∑–æ–≤—ã–π –≤–∞—Ä–∏–∞–Ω—Ç –±–µ–∑ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: –∑–∞–≥–æ–ª–æ–≤–æ–∫ + –∏—Å—Ç–æ—á–Ω–∏–∫
                                safe_link = normalize_url(link or '')
                                message_html = (
                                    f"<b>üì∞ {html_lib.escape(title)}</b>\n\n"
                                    f"–ò—Å—Ç–æ—á–Ω–∏–∫: {short_source_anchor(safe_link)}"
                                )
                                # –¢–æ–∂–µ –ø—ã—Ç–∞–µ–º—Å—è –¥–æ–±–∞–≤–∏—Ç—å —Å–∫–∏–¥–∫–∏
                                game_title = extract_game_title(title)
                                deal_lines = check_top_deals(game_title, MAX_DEALS)
                                if deal_lines:
                                    message_html += "\n\n" + "\n".join(deal_lines)

                            if image_url:
                                caption = message_html
                                if len(caption) > MAX_CAPTION:
                                    caption = caption[:MAX_CAPTION - 1] + '‚Ä¶'
                                for send_attempt in range(1, MAX_RETRIES + 1):
                                    try:
                                        await bot.send_photo(
                                            chat_id=CHANNEL_ID,
                                            photo=image_url,
                                            caption=caption,
                                            parse_mode='HTML'
                                        )
                                        break
                                    except RetryAfter as e:
                                        delay = int(getattr(e, 'retry_after', 5))
                                        logging.warning(f'Telegram –ø—Ä–æ—Å–∏—Ç –ø–æ–¥–æ–∂–¥–∞—Ç—å {delay}—Å (FloodWait). –ü–æ–ø—ã—Ç–∫–∞ {send_attempt}/{MAX_RETRIES}')
                                        await asyncio.sleep(delay)
                                    except TimedOut as e:
                                        logging.warning(f'–°–µ—Ç–µ–≤–∞—è –æ—à–∏–±–∫–∞ Telegram (TimedOut): {e}. –ü–æ–ø—ã—Ç–∫–∞ {send_attempt}/{MAX_RETRIES}')
                                        await asyncio.sleep(BACKOFF_BASE_SEC ** send_attempt)
                                    except NetworkError as e:
                                        logging.warning(f'–°–µ—Ç–µ–≤–∞—è –æ—à–∏–±–∫–∞ Telegram: {e}. –ü–æ–ø—ã—Ç–∫–∞ {send_attempt}/{MAX_RETRIES}')
                                        await asyncio.sleep(BACKOFF_BASE_SEC ** send_attempt)
                                    except Exception as e:
                                        logging.error(f'–û—à–∏–±–∫–∞ –æ—Ç–ø—Ä–∞–≤–∫–∏ —Ñ–æ—Ç–æ: {e}')
                                        break
                            else:
                                for part in split_message(message_html, MAX_TELEGRAM_MESSAGE):
                                    for send_attempt in range(1, MAX_RETRIES + 1):
                                        try:
                                            await bot.send_message(
                                                chat_id=CHANNEL_ID,
                                                text=part,
                                                parse_mode='HTML',
                                                disable_web_page_preview=False
                                            )
                                            break
                                        except RetryAfter as e:
                                            delay = int(getattr(e, 'retry_after', 5))
                                            logging.warning(f'Telegram –ø—Ä–æ—Å–∏—Ç –ø–æ–¥–æ–∂–¥–∞—Ç—å {delay}—Å (FloodWait). –ü–æ–ø—ã—Ç–∫–∞ {send_attempt}/{MAX_RETRIES}')
                                            await asyncio.sleep(delay)
                                        except TimedOut as e:
                                            logging.warning(f'–°–µ—Ç–µ–≤–∞—è –æ—à–∏–±–∫–∞ Telegram (TimedOut): {e}. –ü–æ–ø—ã—Ç–∫–∞ {send_attempt}/{MAX_RETRIES}')
                                            await asyncio.sleep(BACKOFF_BASE_SEC ** send_attempt)
                                        except NetworkError as e:
                                            logging.warning(f'–°–µ—Ç–µ–≤–∞—è –æ—à–∏–±–∫–∞ Telegram: {e}. –ü–æ–ø—ã—Ç–∫–∞ {send_attempt}/{MAX_RETRIES}')
                                            await asyncio.sleep(BACKOFF_BASE_SEC ** send_attempt)
                                        except Exception as e:
                                            logging.error(f'–û—à–∏–±–∫–∞ –ø–æ—Å—Ç–∏–Ω–≥–∞: {e}')
                                            break

                            last_posts[post_key] = True
                            save_last_posts()
                            logging.info(f"–û–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–æ: {title}")
                            news_sent += 1
                        except Exception as e:
                            logging.error(f'–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∑–∞–ø–∏—Å–∏ —Ñ–∏–¥–∞: {e}')
                    break
                except Exception as e:
                    logging.warning(f'–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ RSS {feed_url}: {e}. –ü–æ–ø—ã—Ç–∫–∞ {attempt}/{MAX_RETRIES}')
                    if attempt < MAX_RETRIES:
                        await asyncio.sleep(BACKOFF_BASE_SEC ** attempt)
                    else:
                        logging.error(f'–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å RSS {feed_url} –ø–æ—Å–ª–µ {MAX_RETRIES} –ø–æ–ø—ã—Ç–æ–∫')

def handle_signal(signum, frame):
    logging.info(f'–ü–æ–ª—É—á–µ–Ω —Å–∏–≥–Ω–∞–ª {signum}. –ó–∞–≤–µ—Ä—à–∞–µ–º —Ä–∞–±–æ—Ç—É...')
    stop_event.set()

async def main():
    global last_posts
    last_posts = load_last_posts()

    signal.signal(signal.SIGINT, handle_signal)
    signal.signal(signal.SIGTERM, handle_signal)

    logging.info("–ë–æ—Ç –∑–∞–ø—É—â–µ–Ω. –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –Ω–æ–≤–æ—Å—Ç–∏ –∫–∞–∂–¥—ã–µ 6 —á–∞—Å–æ–≤.")

    schedule.clear()
    schedule.every(POST_INTERVAL_HOURS).hours.do(lambda: asyncio.create_task(fetch_and_post_news()))

    if os.getenv('AUTO_START') == '1':
        try:
            await fetch_and_post_news()
        except Exception as e:
            logging.error(f'–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–µ—Ä–≤–∏—á–Ω–æ–º –∑–∞–ø—É—Å–∫–µ: {e}')

        while not stop_event.is_set():
            schedule.run_pending()
            await asyncio.sleep(1)

        save_last_posts()
        logging.info('–ë–æ—Ç –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω.')
    else:
        logging.info('AUTO_START != 1 ‚Äî –∞–≤—Ç–æ–∑–∞–ø—É—Å–∫ –æ—Ç–∫–ª—é—á—ë–Ω. –°–∫—Ä–∏–ø—Ç –∑–∞–≤–µ—Ä—à–∞–µ—Ç —Ä–∞–±–æ—Ç—É.')

if __name__ == '__main__':
    asyncio.run(main())